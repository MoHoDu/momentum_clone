트랜스포머?

인코더--> 피드포와드 / 멀티헤드어텐션
피드포워드 : 덴스..

어텐션은?
A dog ate the food because it was hungry
--> it과 dog를 어떻게 연관시킬까?
--> 어텐션!!

단어와 단어 사이에 연관성 설명
--> 연관성을 수치로 나타냄
        수치는 좀 이따 

I am good --> 인코더
--> 수치화 : 임베딩
--> 3 X 512로 임베딩

쿼리와 키 : 단어와 단어의 연관성
벨류 : 연관성의 중요도 설명

어떻게 이를 생성? (3 X 512)
--> 각각 wq wk wv
wq: I / am / good x 64
wk: I / am / good x 64
wv: I / am / good x 64

행렬  I   am  good
I    110 90  80
am   70  90  ..
good ... ... .. 

기억의 손실을 막기 위해서 
dk 루트로 나눠줍니다 루트 64는 8

--> 소프트맥스로 변환

소프트맥스 : 사과 사진을 뉴런에다 집어 넣음
    확률 계산 : 사과일 확률 , 귤.., 배...
    다 더했을 때에 1
    원하는 값 : 1, 0, 0

0.9     0.07    0.03
0.025   0.95    0.025
0.21    0.03    0.76

--> 모든 열의 합이 1

어떤 정보가 얼마나 쓸모가 있고, 관련이 있는지 알수가 있음

벨류를 거기다 z를 곱한다고?
이걸 행렬을 만들어서 차원 w를 곱해?

버트? --> 인코더 쌓은거?
GPT? --> ???
뭐 이걸 할 수 있다.

----------------------------------------

디코더에는...
멀티헤드어텐션
피드
마스크

인코더에서 I am dog
--> 3x512 --> wq wk wv를 각각 곱해서 q,k,v행렬 만듦
--> 피드 3번
--> 디코더에 전달 ---> 서브 레이어?????

서브 레이어: 문장에 대한 정보를 알려주는 것

<sos>라는 토큰을 집어넣음 --> Je -->
Je가 함께 들어감 --> 인코더 연산 결과와 함께 
--> La 

--> sos 와 Je, La .....
eos토큰이 나올 때까지 반복

위치 인코딩
- 단어 순서에 따라 문장의 뜻이 달라짐
- 단어 위치는 굉장히 중요한 정보!!
- 기존 rnm lstm과 다르게 
- 트랜스포머? 에서는 한번에 i am good을 집어 넣기 때문에
- 위치 값을 저장할 수 없음

--> 이 논문에서는...뭔지 이해 불가..

마스크된 멀티헤드어텐션
마스크: 단어 하나... n개의 단어에만 선택 집중하고 싶을 때 쓰임
- I에 대해서만 하고 싶을 수 있음
- 행렬에서 신경쓰기 싫은 부분을 -무한대 처리해버림
- sos만 정보를 알고 싶으면 je la는 모두 음의 무한대로 처리해버림
